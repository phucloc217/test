{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "182dngHuZVuQ809U-CKeDgRyoIErxmND-",
      "authorship_tag": "ABX9TyNofUmQwzfXyA9NLz7+Dp1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/phucloc217/test/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7c5BSANMybf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0JghuB7NIz9",
        "outputId": "0e285eea-849f-4d50-e2ab-8f3bb0eab618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Thống kê các word xuất hiện ở tất cả các nhãn\n",
        "total_label = 18\n",
        "vocab = {}\n",
        "label_vocab = {}\n",
        "for line in open('/content/drive/MyDrive/data.txt'):\n",
        "    words = line.split()\n",
        "    # lưu ý từ đầu tiên là nhãn\n",
        "    label = words[0]\n",
        "    if label not in label_vocab:\n",
        "        label_vocab[label] = {}\n",
        "    for word in words[1:]:\n",
        "        label_vocab[label][word] = label_vocab[label].get(word, 0) + 1\n",
        "        if word not in vocab:\n",
        "            vocab[word] = set()\n",
        "        vocab[word].add(label)\n",
        "\n",
        "count = {}\n",
        "for word in vocab:\n",
        "    if len(vocab[word]) == total_label:\n",
        "        count[word] = min([label_vocab[x][word] for x in label_vocab])\n",
        "        \n",
        "sorted_count = sorted(count, key=count.get, reverse=True)\n",
        "for word in sorted_count[:100]:\n",
        "    print(word, count[word])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJ_F0RIwOl1u",
        "outputId": "5374b357-df90-4055-a7ef-6b1a29689fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "và 14255\n",
            "của 13177\n",
            "là 9983\n",
            "có 9162\n",
            "được 9131\n",
            "trong 8654\n",
            "một 7575\n",
            "cho 7483\n",
            "với 7195\n",
            "không 6591\n",
            "các 6300\n",
            "người 6088\n",
            "khi 6011\n",
            "này 5301\n",
            "đến 5165\n",
            "để 5123\n",
            "đã 4431\n",
            "nhiều 4167\n",
            "trên 3842\n",
            "từ 3820\n",
            "vào 3617\n",
            "đó 3207\n",
            "những 3097\n",
            "ở 2943\n",
            "ra 2767\n",
            "tại 2756\n",
            "vn 2738\n",
            "lại 2673\n",
            "cũng 2622\n",
            "phải 2615\n",
            "còn 2589\n",
            "theo 2565\n",
            "nhưng 2545\n",
            "zing 2519\n",
            "về 2170\n",
            "sau 2047\n",
            "làm 1983\n",
            "lên 1921\n",
            "hơn 1755\n",
            "đây 1692\n",
            "năm 1654\n",
            "sẽ 1597\n",
            "chỉ 1569\n",
            "cả 1534\n",
            "cùng 1490\n",
            "ngày 1484\n",
            "như 1467\n",
            "mà 1458\n",
            "vẫn 1386\n",
            "đi 1373\n",
            "2 1358\n",
            "mới 1357\n",
            "khác 1300\n",
            "3 1296\n",
            "hai 1278\n",
            "qua 1250\n",
            "bạn 1138\n",
            "bên 1137\n",
            "1 1136\n",
            "khiến 1122\n",
            "5 1114\n",
            "lần 1051\n",
            "mình 1045\n",
            "lớn 1030\n",
            "bị 1022\n",
            "biết 1013\n",
            "trước 1000\n",
            "rất 991\n",
            "tới 968\n",
            "bằng 948\n",
            "mang 929\n",
            "nên 897\n",
            "4 896\n",
            "đang 870\n",
            "nước 863\n",
            "cách 863\n",
            "việt_nam 861\n",
            "đầu 849\n",
            "10 847\n",
            "việc 840\n",
            "nếu 835\n",
            "vừa 826\n",
            "thấy 824\n",
            "hàng 807\n",
            "vì 806\n",
            "ảnh 799\n",
            "đều 796\n",
            "nhau 788\n",
            "thời_gian 787\n",
            "sự 764\n",
            "anh 760\n",
            "6 737\n",
            "nhất 733\n",
            "ngoài 720\n",
            "điều 712\n",
            "hay 706\n",
            "giữa 699\n",
            "số 699\n",
            "từng 697\n",
            "thêm 692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loại stopword khỏi dữ liệu\n",
        "# lưu file dùng về sau\n",
        "stopword = set()\n",
        "with open('stopwords.txt', 'w') as fp:\n",
        "    for word in sorted_count[:100]:\n",
        "        stopword.add(word)\n",
        "        fp.write(word + '\\n')\n",
        "    \n",
        "def remove_stopwords(line):\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)\n",
        "    \n",
        "    \n",
        "with open('news_categories.prep', 'w') as fp:\n",
        "    for line in open('/content/drive/MyDrive/data.txt'):\n",
        "        line = remove_stopwords(line)\n",
        "        fp.write(line + '\\n')"
      ],
      "metadata": {
        "id": "_WvNK61GPrhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "test_percent = 0.2\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "\n",
        "for line in open('news_categories.prep'):\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
        "\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('train.txt', 'w') as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('test.txt', 'w') as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "print(X_train[0], y_train[0], '\\n')\n",
        "print(X_test[0], y_test[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMKz3IsrQIVK",
        "outputId": "f176b668-89fc-40ad-9db9-b0b73216537c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__label__công_nghệ', '__label__du_lịch', '__label__giáo_dục', '__label__giải_trí', '__label__kinh_doanh', '__label__nhịp_sống', '__label__phim_ảnh', '__label__pháp_luật', '__label__sống_trẻ', '__label__sức_khỏe', '__label__thế_giới', '__label__thể_thao', '__label__thời_sự', '__label__thời_trang', '__label__xe_360', '__label__xuất_bản', '__label__âm_nhạc', '__label__ẩm_thực'] \n",
            "\n",
            "long_nhật chê chuyện phương_thanh cứu siu_black sao_việt cái nhìn ngược hoàn_toàn đông long_nhật rằng phương_thanh mọi chuyện rối_tung long_nhật chê chuyện phương_thanh cứu siu_black cái nhìn ngược hoàn_toàn đông long_nhật rằng phương_thanh mọi chuyện rối_tung chuyện siu_black vỡ_nợ thực_sự tạo cơn chấn_động giới giải_trí thay_vì máu_lửa sân_khấu giọng ca núi_rừng xuất_hiện gương_mặt bơ_phờ khóe miệng nở nụ_cười gượng_gạo phương_thanh duy_nhất phát_ngôn báo_chí thừa_nhận khoản nợ tỷ đồng siu_black hoàn_toàn im_lặng thay chị trả_lời tất_cả thắc_mắc giới truyền_thông chính nữ ca_sĩ hâm_mộ gọi cái tên chanh tin phương_thanh chấp_nhận đứng_mũi_chịu_sào thân nhìn chị đầy ái_ngại khen chị tốt_tính lúc khó_khăn ai ai bè cho_rằng giọng ca mèo hoang_dại quá chuyện nhảy chịu khổ ai đuổi suy_nghĩ đông cái nhìn hoạt_động giới showbiz nam ca_sĩ long_nhật đưa nhìn_nhận hoàn_toàn khác_biệt lời nam ca_sĩ huế thân_thiết chị siu sự_cố phương_thanh rối_tung mọi thứ chuyện phương_thanh nhận quản_lý siu_black trả_lời báo_chí chứ tin nóng như_thế làm_sao tôi bỏ_qua mọi nhìn_nhận hành_động nghĩa_hiệp thật_sự tôi nực_cười muốn đứng lo chuyện nợ_nần trước_hết tiền đủ sức trả nợ người_ta thì_thôi giúp giúp đừng nửa_chừng như_vậy ý phương_thanh tiền bản_thân cô ấy thời đỉnh_cao giờ cô ấy đóng phim kiếm tiền thù_lao đóng phim thấp lắm bộ phim 40 tập đóng tháng trời nhận khoản tiền ca_sĩ hát đêm ít_ra chị ấy dám đứng chịu_trận đâu ai gan chuyện vấn_đề nợ_nần tiền_bạc tiền giúp người_ta nói cái gì bây_giờ khả_năng sao nói tôi sẵn_sàng trả giúp siu_black siu hát trả nợ tôi như_thế chứ toàn nói chung_chung trả_lời rùm_beng báo_chí gì tổ nói nói lằng_nhằng mọi thứ thôi thương_siu như_thế mười_hại siu trả nợ siu phương_thanh khẳng_định giúp siu hát kiếm tiền một_cách đấy chứ chị siu bây_giờ nói rõ_ràng chị ấy tinh_thần hát siu_hát cháy hết_mình chị ấy hát cảm_xúc giờ tâm_trạng đâu cháy nợ_nần bủa_vây như_thế hay_là muốn hát dưới người_ta đòi nợ đông khán_giả tôi chắc_chắn cần bày treo băng_rôn chủ_nợ kéo liền ý phương_án hoàn_toàn hiệu_quả phương_thanh theo_đuổi việc_làm thực_tế cô ấy muốn anh_hùng muốn ra_tay nghĩa_hiệp càng sự_việc rối giờ phương_thanh bắt_đầu cảm_thấy mệt_mỏi muốn bỏ_của_chạy_lấy_người vậy gì giúp chị siu trường_hợp tôi thân_thiết quý_mến chị siu chúng_tôi mấy chục nay thời chưa nào nổi_tiếng tôi giờ muốn giúp siu tất_cả ngồi xuống bàn_bạc kỹ chứ không_thể sồn_sồn phương_thanh bấy_lâu_nay có_thể nghệ_sĩ đứng tổ_chức đêm nhạc bán vé lấy cát xê toàn_bộ tiền thu dùng ủng_hộ chị siu cần giúp_đỡ chính chân_thành tôi tin chị siu có_thể vượt khó_khăn trí_thức trẻ 3 \n",
            "\n",
            "thư_an_nguy gửi toàn_shinoda trái_tim muốn vỡ tung sống trẻ trưa hôm_nay 24 8 an_nguy đầu_tiên viết dòng tâm_sự cảm_xúc toàn_shinoda đột_ngột 27 7 an_nguy lập_tức quay mỹ vài dự tang_lễ toàn_shinoda đột_ngột bạn_trai an_nguy hoàn_toàn suy_sụp cô chưa nào tâm_sự mất_mát thời_điểm hiện_tại gần tháng toàn_shinoda mất an_nguy bộc_bạch cảm_xúc vui buồn tức_giận đau_khổ người_thân lặng nhìn toàn_shinoda giờ hỏa_táng trưa 27 7 linh_cữu vlogger toàn_shinoda di_chuyển đài_hóa_thân hoàn_vũ_văn_điển hà_nội 16h lễ hỏa_táng kết_thúc đau_thương lời tâm_sự an_nguy đăng_tải 30 phút thu_hút 80 000 lượt like thích ủng_hộ đồng_cảm cộng_đồng mạng an_nguy viết em giận em nghĩ giận đời chẳng bao_giờ tha_thứ em giận mãi im_lặng mãi chẳng dỗ em quay nữ hèn nữ lúc_nào sợ giận sợ thương em nữa nghe em chẳng tin ngàn em chẳng_thể tin chuyện gì xảy nửa vòng trái_đất em kẻ mất chân mất tay gọi chạy nhà em ngồi cái máy_tính nực_cười em trút giận tất_cả báo tin em em gào họ chừng nào gặp tin rồi họ gặp em ngồi đọc dòng chữ màn_hình đầu_tiên em cảm_thấy bất_lực thế cứ tuột dần khỏi tay em một_cách mơ_hồ em khóc em hiểu loại động_vật máu lạnh nào em em không_thể khóc em câm_lặng thôi chia_sẻ an_nguy facebook em kẻ hèn_nhát em muốn quay em sợ sợ khóc sợ người_ta thương_hại em em không_bao_giờ muốn ai thương_hại trên_hết em sợ đối_diện sự_thật sợ nhìn nằm em sợ em gọi không_bao_giờ nghe câu trả_lời nữa em tự_nhủ giấc_mơ giấc_mơ quá dài quá thật trở_về nghĩa_là tỉnh_giấc cơn ác_mộng trở_thành hiện_thực em suy_nghĩ ác_độc em nói em bảo_vệ bất_kì kẻ nào tổn_thương lợi_dụng động em thì động anh_em yên cười kệ rồi chúng_nó hiểu thôi à chúng_nó hiểu đâu rồi chúng_nó quyết chịu hiểu đâu em kệ em muốn giết chết kẻ thêu_dệt muốn tự tay giết hết chúng_nó súng vậy nhẹ_nhàng quá dao không_chỉ nhát chục trăm hình nữa em nói rồi cần em sợ bố con thằng nào hết em sống giận_dữ em giận ông trời giận giận giận tất_cả mọi xung_quanh giận ông trời sao đem mất giận sao bỏ mọi giận sao giữ người_ta viết em giận làm_sao viết cứ quay người_ta viết nữa em giận mấy hôm dám quên tức_giận vô_lý lúc_nào em muốn đập muốn phá tức_giận ấy đè nặng em kẻ nắm lấy trái_tim em bóp nghiến lấy nó em gì em sống cuộc_sống bình_thường ăn ngủ cười nói trái_tim muốn vỡ tung đêm ngồi dưới đường nhìn ban công phòng em gọi chờ tiếng xuống đợi một_tí em đợi mãi chẳng ai xuất_hiện em nghĩ rằng cần em đợi bao_lâu thôi em đợi rồi em nhận_ra cái bao_lâu ấy không_bao_giờ nữa em tiếp_tục mất thứ tốt_đẹp xảy có_điều mất mãi_mãi em tin kiếp kiếp thật em chắc_chắn gặp tiếp_tục em thành lâm_việt_anh việt lúc_nào lặng_lẽ quan_tâm giúp_đỡ bọn em bọn em đời quên vui_vẻ đau_đớn nhắc bây_giờ em ích_kỉ đòi nữa thanh_thản vướng_bận gì đừng lo em em yếu_đuối muốn mạnh_mẽ cần đường tình dang_dở an_nguy toàn_shinoda chính_thức công_khai tình_yêu tháng an_nguy gái toàn shinoda lặng_lẽ đột_ngột bạn_trai 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = \"models\"\n",
        "\n",
        "import os\n",
        "if not os.path.exists(MODEL_PATH):\n",
        "    os.makedirs(MODEL_PATH)"
      ],
      "metadata": {
        "id": "wQRQE_ofQY8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "\n",
        "# Lưu model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"naive_bayes.pkl\"), 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utrbDrWdQdH9",
        "outputId": "ab6a5ff7-c5b0-47e1-a889-0b082af2d42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Naive Bayes in 17.101754426956177 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "    \n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print('Done training Linear Classifier in', train_time, 'seconds.')\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(MODEL_PATH, \"linear_classifier.pkl\"), 'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWWqlqSxaNR3",
        "outputId": "fd390f27-94dc-4b64-af38-897f823a2a48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done training Linear Classifier in 233.57364964485168 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Naive Bayes\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Naive Bayes, Accuracy =', np.mean(y_pred == y_test))\n",
        "\n",
        "# Linear Classifier\n",
        "model = pickle.load(open(os.path.join(MODEL_PATH,\"linear_classifier.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Linear Classifier, Accuracy =', np.mean(y_pred == y_test))\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzNDSa7SbsVT",
        "outputId": "1a165e86-2dcf-44a9-9e7a-949ab854135d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes, Accuracy = 0.8324764353041988\n",
            "Linear Classifier, Accuracy = 0.883140531276778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xem kết quả trên từng nhãn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nb_model = pickle.load(open(os.path.join(MODEL_PATH,\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVbFsuracnfS",
        "outputId": "5a2f2672-8904-4811-aeb3-2b249204e838"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     precision    recall  f1-score   support\n",
            "\n",
            " __label__công_nghệ       0.91      0.92      0.92       532\n",
            "   __label__du_lịch       0.79      0.88      0.83       551\n",
            "  __label__giáo_dục       0.82      0.88      0.85       528\n",
            "  __label__giải_trí       0.59      0.74      0.66       487\n",
            "__label__kinh_doanh       0.78      0.84      0.81       498\n",
            " __label__nhịp_sống       0.85      0.49      0.62       497\n",
            "  __label__phim_ảnh       0.90      0.76      0.82       525\n",
            " __label__pháp_luật       0.90      0.92      0.91       543\n",
            "  __label__sống_trẻ       0.62      0.64      0.63       510\n",
            "  __label__sức_khỏe       0.79      0.88      0.83       496\n",
            "  __label__thế_giới       0.91      0.83      0.87       549\n",
            "  __label__thể_thao       0.95      0.95      0.95       508\n",
            "   __label__thời_sự       0.82      0.77      0.79       496\n",
            "__label__thời_trang       0.86      0.77      0.81       521\n",
            "    __label__xe_360       0.97      0.94      0.96       502\n",
            "  __label__xuất_bản       0.87      0.93      0.90       519\n",
            "   __label__âm_nhạc       0.83      0.87      0.85       554\n",
            "   __label__ẩm_thực       0.90      0.94      0.92       520\n",
            "\n",
            "           accuracy                           0.83      9336\n",
            "          macro avg       0.84      0.83      0.83      9336\n",
            "       weighted avg       0.84      0.83      0.83      9336\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPRXGLkUdesH",
        "outputId": "a1a105a1-8990-49ac-bc3a-3a335dfaa8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: underthesea in /root/.local/lib/python3.8/site-packages (6.1.4)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /root/.local/lib/python3.8/site-packages (from underthesea) (0.9.9)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.2.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from underthesea) (6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from underthesea) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from underthesea) (2.25.1)\n",
            "Requirement already satisfied: underthesea-core==1.0.0 in /root/.local/lib/python3.8/site-packages (from underthesea) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from underthesea) (1.0.2)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from underthesea) (3.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->underthesea) (2022.6.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->underthesea) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install sklearn-pycrfsuite\n",
        "! pip install underthesea==6.0.3\n",
        "import regex as re\n",
        "\n",
        "from underthesea import word_tokenize\n",
        " \n",
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        " \n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Hàm chuyển Unicode dựng sẵn về Unicde tổ hợp (phổ biến hơn)\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)\n",
        "\n",
        "bang_nguyen_am = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "                  ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "                  ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "                  ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "                  ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "                  ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "                  ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "                  ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "                  ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "                  ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "                  ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "                  ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "bang_ky_tu_dau = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "nguyen_am_to_ids = {}\n",
        "\n",
        "for i in range(len(bang_nguyen_am)):\n",
        "    for j in range(len(bang_nguyen_am[i]) - 1):\n",
        "        nguyen_am_to_ids[bang_nguyen_am[i][j]] = (i, j)\n",
        "\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cw = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        # print(cw)\n",
        "        if len(cw) == 3:\n",
        "            cw[1] = chuan_hoa_dau_tu_tieng_viet(cw[1])\n",
        "        words[index] = ''.join(cw)\n",
        "    return ' '.join(words)\n",
        "\n",
        "def remove_html(txt):\n",
        "    return re.sub(r'<[^>]*>', '', txt)"
      ],
      "metadata": {
        "id": "DC3q1ocXdJLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST**"
      ],
      "metadata": {
        "id": "6_XLqDzKdB1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocess(document):\n",
        "    # xóa html code\n",
        "    document = remove_html(document)\n",
        "    # chuẩn hóa unicode\n",
        "    document = convert_unicode(document)\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    # tách từ\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # đưa về lower\n",
        "    document = document.lower()\n",
        "    # xóa các ký tự không cần thiết\n",
        "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
        "    # xóa khoảng trắng thừa\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "    return document"
      ],
      "metadata": {
        "id": "AAS7HJjKdDpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document = \"Nền kinh tế Việt Nam là nền kinh tế thị trường định hướng xã hội chủ nghĩa\"\n",
        "\n",
        "document = text_preprocess(document)\n",
        "document = remove_stopwords(document)\n",
        "\n",
        "label = nb_model.predict([document])\n",
        "print('Nhãn được xác định:', label_encoder.inverse_transform(label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yodXcPlffN4a",
        "outputId": "8afd08c7-48b1-4767-f969-1b1531b6b2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nhãn được xác định: ['__label__kinh_doanh']\n"
          ]
        }
      ]
    }
  ]
}